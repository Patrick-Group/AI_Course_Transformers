# AI Course from Pitrick Chiang
## Introduction
This course will give you some edge information about Artificial Intelligence as well as some fundamental knowledge regards transformers architecture. Hope you guys enjoy it.
Special thanks to [JsonBorn7](https://github.com/JsonBorn7) for course materials.

### Course_1
Course_1 will give you a guide to use the huggingface tools in Natural Language Processing (NLP). This concise course is designed to equip you with the essential skills to navigate and utilize HuggingFace's innovative platforms, preparing you for advanced NLP projects. Dive in and start transforming the way you work with AI.

### Course_2
Course_2 will help you dive into the basics of how transformers get data ready for action. We'll cover how tokenizer and embedding work together to prep data, making it just right for transformers to use. It's all about turning words into a form that computers can understand and work with.

### Course_3
Course_3 will assist you explore the full structure of realized transformers in this brief overview, where we'll cover everything from tokenization to attention mechanisms. Get a clear view of how these components work together to do text processing.

### related papers
[OneBit: Towards Extremely Low-bit Large Language Models](https://arxiv.org/abs/2402.11295)

[Efficient Large Language Models: A Survey](https://arxiv.org/abs/2312.03863)

[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)

[BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453)

[]()


## Week2 Homework 
Your homework is run the code in course_3. I set a trap inside the code. Your target will be:
1. successfully run the code.
2. successfully train the model. (loss will drop by training.)
3. be able to replace the training materials.
4. modify the model.

## Week3 Homework 
Now you have the full version runable code, do some hand adjustment and see how these adjustments effect Hardware.
Adjust the following config|structure of transformers model:
1. number of heads
2. transformer iterations
3. context length
4. embeddings
5. quantization (pending)


## Week4 Content
A new hand-made ViT model with MNIST datasets is ready to use. Feel free to use it and learn the structure of ViT. There is no homework left.